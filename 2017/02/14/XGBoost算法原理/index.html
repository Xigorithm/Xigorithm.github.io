<!DOCTYPE html>
<html>
  <!DOCTYPE html>
<html lang="zh-Hans">
<link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">

<script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
<script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  
  <title>XGBoost算法原理 - July</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  
  <meta name="keywords" content=>
  
    <meta name="description" content="每天进步一点点">
  
  
    <link rel="shortcut icon" type="image/x-icon" href="/images/icon/football2.png?v=1.02">
  
  
    <link rel="alternate" href="/atom.xml " title="July" type="application/atom+xml">
  

  
<script src="/js/fancybox.js"></script>


  
<link rel="stylesheet" href="/css/style.css">


<meta name="generator" content="Hexo 5.3.0"></head>
  <body>
    <div class="container">
      
<header class="header">
  <div class="blog-title">
    <a href="/" class="logo">July</a>
    <div class="subtitle">life feeds on negative entropy.</div>
  </div>
  <nav class="navbar">
    <ul class="menu">
      
        <li class="menu-item">
          <a href="/" class="menu-item-link">主页</a>
        </li>
      
        <li class="menu-item">
          <a href="/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD" class="menu-item-link">人工智能</a>
        </li>
      
        <li class="menu-item">
          <a href="/categories/%E6%88%91%E7%9A%84%E4%B9%A6%E5%8D%95" class="menu-item-link">我的书单</a>
        </li>
      
        <li class="menu-item">
          <a href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94" class="menu-item-link">生活随笔</a>
        </li>
      
        <li class="menu-item">
          <a href="/about" class="menu-item-link">关于</a>
        </li>
      
    </ul>
  </nav>

</header>


<article class="post">
  <h1 class="article-title"></h1>
  <div class="post-title">
    <h1 class="file-title">XGBoost算法原理</h1>
  </div>
   
  <div class="post-content">
    <blockquote>
<p>XGBoost(eXtreme Gradient Boosting)是GBDT的一种改进形式，具有很好的性能，在各大比赛中大放异彩。</p>
</blockquote>
<h1 id="1-XGBoost的算法原理"><a href="#1-XGBoost的算法原理" class="headerlink" title="1 XGBoost的算法原理"></a>1 XGBoost的算法原理</h1><p>经过k轮迭代后，GBDT/GBRT的损失函数可写成$L(y,f_k(\boldsymbol{x}))$，将$f_k(\boldsymbol{x})$视为变量，对$L(y,f_k(\boldsymbol{x}))$在$f_{k-1}(\boldsymbol{x})$进行二阶泰勒展开：</p>
<script type="math/tex; mode=display">
L(y,f_k(\boldsymbol{x}))\approx L(y,f_{k-1}(\boldsymbol{x}))+\frac{\partial L(y,f_{k-1}(\boldsymbol{x}))}{\partial f_{k-1}(\boldsymbol{x})}[f_k(\boldsymbol{x})-f_{k-1}(\boldsymbol{x})]+\frac{1}{2} \frac{\partial ^2L(y,f_{k-1})}{\partial ^2f_{k-1}(\boldsymbol{x})}[f_k(\boldsymbol{x})-f_{k-1}(\boldsymbol{x})] ^2 \tag{1.1}</script><p>取$g=\frac{\partial L(y,f_{k-1}(\boldsymbol{x}))}{\partial f_{k-1}(\boldsymbol{x})},h=\frac{\partial ^2L(y,f_{k-1}(\boldsymbol{x}))}{\partial f_{k-1}^2(\boldsymbol{x})}  $，代入上式，得到：</p>
<script type="math/tex; mode=display">
L(y,f_k(\boldsymbol{x}))\approx L(y,f_{k-1}(\boldsymbol{x}))+g[f_k(\boldsymbol{x})-f_{k-1}(\boldsymbol{x})]+\frac{1}{2}h[f_k(\boldsymbol{x})-f_{k-1}(\boldsymbol{x})]^2 \tag{1.2}</script><p>在GBDT中，利用前向分布算法，有$f_k(\boldsymbol{x})-f_{k-1}(\boldsymbol{x})=T_k(\boldsymbol{x})$，代入上式，得到：</p>
<script type="math/tex; mode=display">
L(y,f_k(\boldsymbol{x}))\approx L(y,f_{k-1}(\boldsymbol{x}))+gT_k(\boldsymbol{x})+\frac{1}{2}h[T_k{\boldsymbol{x}}]^2 \tag{1.3}</script><p>上面的损失函数目前是针对一个样本数据而言，对于整体样本，其损失函数为：</p>
<script type="math/tex; mode=display">
L\approx  {\textstyle \sum_{i=1}^{N}}L(y_i,f_k{\boldsymbol{x}_i}) =\sum_{i=1}^{N} [L(y_i,f_{k-1}(\boldsymbol{x}_i))+g_iT_k(\boldsymbol{x}_i)+\frac{1}{2}h_i[T_k{\boldsymbol{x_i}}]^2] \tag{1.4}</script><p>等式右边中第一项$L(y_i,f_{k-1}(\boldsymbol{x}_i))$只与前k-1轮有关，第k轮优化中可将该项视为常数。另外，在GBDT的损失函数上再加上一项与第k轮的基学习器CART决策树相关的正则化项$\omega (T_k(x))$防止过拟合，可得到新的第k轮迭代时的等价损失函数：</p>
<script type="math/tex; mode=display">
L_k=\sum_{i=1}^{N}[g_iT_k(\boldsymbol{x}_i)+\frac{1}{2}h_i[T_k(\boldsymbol{x}_i)]^2+\Omega (T_k(\boldsymbol{x})) ]  \tag{1.5}</script><p>上式就是XGBoost模型的损失函数。</p>
<p>假设第k棵CART回归树其对应的叶子区域样本子集为$D_{k1},D_{k2},…,D_{kT}$，且第j个小单元$D_{kj}$中仍然包含$N_{kj}$个样本数据，则计算每个小单元里面的样本的输出均值为：</p>
<script type="math/tex; mode=display">
\bar{c}_{kj}=\frac{1}{N_{kj}}\sum_{x_i\in D_{kj}}^{}y_i   \tag{1.6}</script><p>得到:</p>
<script type="math/tex; mode=display">
T_k(\boldsymbol{x})=\sum_{j=1}^{T}\bar{c}_{kj}I(x_i\in D_{kj})   \tag{1.7}</script><p>正则化项的构造为：</p>
<script type="math/tex; mode=display">
\Omega (T_k(\boldsymbol{x}))=\gamma T+\frac{1}{2}\lambda \sum_{j=1}^{T}\bar{c}_{kj}^2 \tag{1.8}</script><p>其中，参数T为$T_k(x)$决策树的叶子节点的个数，参数$\bar{c}_{kj},j=1,2,…,T $是第$j$个叶子节点的输出均值，$\gamma ,\lambda $是两个权衡因子。叶子节点的数量及其权重因子一起用来控制决策树模型的复杂度。</p>
<h1 id="2-XGBoost与GBDT的比较"><a href="#2-XGBoost与GBDT的比较" class="headerlink" title="2 XGBoost与GBDT的比较"></a>2 XGBoost与GBDT的比较</h1>
  </div>
  <div class="post-footer">
    

    
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
<script type="text/javascript" src="http://mathjax.josephjctang.com/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>

    

    <a href="#top" class="top">Back to Top</a>
  </div>
</article>
<footer>
  &copy; 2017-2023
  <span class="author">
    July
  </span>
</footer>
    </div>
  </body>
</html>