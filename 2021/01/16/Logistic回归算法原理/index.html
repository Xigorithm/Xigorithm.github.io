<!DOCTYPE html>
<html>
  <!DOCTYPE html>
<html lang="zh-Hans">
<link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css">

<script src="https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js"></script>
<script src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js"> </script>

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  
  <title>Logistic回归算法原理 - July</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  
  <meta name="keywords" content=>
  
    <meta name="description" content="每天进步一点点">
  
  
    <link rel="shortcut icon" type="image/x-icon" href="/images/icon/football2.png?v=1.02">
  
  
    <link rel="alternate" href="/atom.xml " title="July" type="application/atom+xml">
  

  
<script src="/js/fancybox.js"></script>


  
<link rel="stylesheet" href="/css/style.css">


<meta name="generator" content="Hexo 5.3.0"></head>
  <body>
    <div class="container">
      
<header class="header">
  <div class="blog-title" float="left">
    <a href="/" class="logo">July</a>
    <div class="subtitle">life feeds on negative entropy.</div>
  </div>
  <nav class="navbar">
    <ul class="menu">
      
        <li class="menu-item">
          <a href="/" class="menu-item-link">主页</a>
        </li>
      
        <li class="menu-item">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="menu-item-link">机器学习</a>
        </li>
      
        <li class="menu-item">
          <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" class="menu-item-link">深度学习</a>
        </li>
      
        <li class="menu-item">
          <a href="/categories/%E6%88%91%E7%9A%84%E4%B9%A6%E5%8D%95" class="menu-item-link">书单</a>
        </li>
      
        <li class="menu-item">
          <a href="/categories/%E7%94%9F%E6%B4%BB%E9%9A%8F%E7%AC%94" class="menu-item-link">生活随笔</a>
        </li>
      
        <li class="menu-item">
          <a href="/about" class="menu-item-link">关于</a>
        </li>
      
    </ul>
  </nav>

</header>


<article class="post">
  <h1 class="article-title"></h1>
  <div class="post-title">
    <h1 class="file-title">Logistic回归算法原理</h1>
  </div>
   
  <div class="post-content">
    <blockquote>
<p>Logistic回归模型就是将线性回归的结果输入一个sigmoid函数，将回归值映射到0~1，表示输出为类别的概率。</p>
</blockquote>
<h1 id="1-Logistic回归模型"><a href="#1-Logistic回归模型" class="headerlink" title="1. Logistic回归模型"></a>1. Logistic回归模型</h1><p>&ensp;&ensp;&ensp;&ensp;线性回归表达式如下：</p>
<script type="math/tex; mode=display">
z_i=w\cdot x_i+b</script><p>$x_i$是第$i$个样本的$N$个特征组成的特征向量，$w$为$N$个特征对应的特征权重组成的向量，$b$是偏置常数。</p>
<p>Sigmoid函数：</p>
<script type="math/tex; mode=display">
y_i = \frac{1}{1+e^{-z_i}}</script><p>其中$z_i$是自变量，$y_i$是因变量，$e$是自然常数。</p>
<p>线性回归的结果套一个sigmoid函数就能得到Logistic回归的结果，即：</p>
<script type="math/tex; mode=display">
y_i = \frac{1}{1+e^{-z_i}}=\frac{1}{1+e^{-(w\cdot x_i+b)}}</script><p>如果我们将$y_i=1$视为$x_i$作为正例的可能性，即：</p>
<script type="math/tex; mode=display">
p(y_i=1|x_i)=\frac{1}{1+e^{-(w\cdot x_i+b)}}=\frac{e^{w\cdot x_i+b}}{1+e^{w\cdot x_i+b}}</script><p>那么反例$y_i=0$的可能性为:</p>
<script type="math/tex; mode=display">
p(y_i=0|x_i)=1-p(y_i=1|x_i)=\frac{1}{1+e^{w\cdot x_i+b}}</script><p>定义两者的比值$\frac{p(y_i=1|x_i)}{p(y_i=0|x_i)}$为“概率”，对其取对数得到“对数概率”：</p>
<script type="math/tex; mode=display">
ln\frac{p(y_i=1|x_i)}{p(y_i=0|x_i)}=w\cdot x_i+b</script><p>可见对数概率的结果正好是线性回归的预测结果$w\cdot x_i+b$，因此，Logistic回归的本质其实就是用线性回归的预测结果$w\cdot x_i+b$去逼近真实标记的对数概率$ln\frac{y}{1-y}$，实际上这也是Logistic回归也被称为“对数概率回归”的原因。</p>
<h1 id="2-Logistic回归学习策略"><a href="#2-Logistic回归学习策略" class="headerlink" title="2. Logistic回归学习策略"></a>2. Logistic回归学习策略</h1><p>&ensp;&ensp;&ensp;&ensp;在上面我们已经知道正例和反例的概率表达式，接下来则要构造似然函数，将其转换为一个优化问题来求解$w$和$b$。</p>
<p>对给定数据集$T={(x_1,y_1),(x_2,y_2),…,(x_M,y_M)}$，定义似然函数：</p>
<script type="math/tex; mode=display">
L(w,b)=\prod_{i=1}^{M}[p(y_i=1|x_i)]^{y_i}[1-p(y_i=1|x_i)] ^{1-y_i}</script><p>取对数，得到对数似然函数：</p>
<script type="math/tex; mode=display">\begin{aligned}
lnL(w,b) &= \sum_{i=1}^{M} {y_i\cdot ln[p(y_i=1|x_i)]+(1-y_i)ln[1-p(y_i=1|x_i)]}\\ &= \sum_{i=1}^{M}{y_i\cdot ln\frac{e^{w\cdot x_i+b}}{1+e^{w\cdot x_i+b}}+(1-y_i)\cdot ln\frac{1}{1+e^{}w\cdot x_i+b} }\\ &= \sum_{i=1}^{M}{y_i\cdot (w\cdot x_i+b)-y_i\cdot ln(1+e^{wx_i+b})+(y_i-1)\cdot ln(1+e^{wx_i+b})}\\ &= \sum_{i=1}^{M}{y_i\cdot (w\cdot x_i+b)-ln(1+e^{w\cdot x_i+b})}
\end{aligned}</script><p>我们需要使每个样本属于其真实标记的概率越大越好，即最大对数似然函数：</p>
<script type="math/tex; mode=display">
\max_{w,b}\sum_{i=1}^{M}{y_i\cdot (w\cdot x_i+b)-ln(1+e^{w\cdot x_i+b})}</script><p>可以用梯度下降或牛顿法来求解该优化问题。</p>
<h1 id="3-Logistic回归优化算法"><a href="#3-Logistic回归优化算法" class="headerlink" title="3. Logistic回归优化算法"></a>3. Logistic回归优化算法</h1><h2 id="3-1-算法流程"><a href="#3-1-算法流程" class="headerlink" title="3.1 算法流程"></a>3.1 算法流程</h2><p>选用批量梯度下降算法，现分别对权重矩阵$w$和偏置常数$b$求偏导：</p>
<script type="math/tex; mode=display">\begin{aligned}
\frac{\partial }{\partial w}lnL(w,b)&=\frac{\partial }{\partial w}\sum_{i=1}^{M}{y_i\cdot (w\cdot x_i+b)-ln(1+e^{}wx_i+b)}\\ &= \sum_{i=1}^{M}(y_i-\frac{e^{wx_i+b}}{1+e^{wx_i+b}} )x_i
\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}
\frac{\partial }{\partial b}lnL(w,b) &= \frac{\partial }{\partial b}\sum_{i=1}^{M}{y_i\cdot (w\cdot x_i+b)-ln(1+e^{}wx_i+b)}\\ &= \sum_{i=1}^{M}(y_i-\frac{e^{wx_i+b}}{1+e^{wx_i+b}} )
\end{aligned}</script><p>每次随机选取一个样本点$(x_i,y_i)$，对$w,b$进行一次更新，得到迭代公式：</p>
<script type="math/tex; mode=display">\begin{aligned}
w\gets w+\eta (y_i-\frac{e^{wx_i+b}}{1+e^{wx_i+b}} )x_i
\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}
b\gets b+\eta (y_i-\frac{e^{wx_i+b}}{1+e^{wx_i+b}} )
\end{aligned}</script><p>Logistic回归流程：</p>
<p>输入：训练集$T={(x_1,y_1),(x_2,y_2),…,(x_M,y_M)}$，学习率$\eta$;</p>
<p>输出：Logistic回归模型$h(x)=\frac{1}{1+e^{-(wx_i+b)}}$。</p>
<p>第1步：选取初始值向量$w$和偏置常数$b$;</p>
<p>第2步：在训练集中随机选取数据$(x_i,y_i)$，根据上面的迭代公式进行参数更新；</p>
<p>第3步：重复步骤2，直至模型满足训练要求。</p>

  </div>
  <div class="post-footer">
    

    
      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    

    <a href="#top" class="top">Back to Top</a>
  </div>
</article>
<footer>
  &copy; 2021-2023
  <span class="author">
    July
  </span>
</footer>
    </div>
  </body>
</html>